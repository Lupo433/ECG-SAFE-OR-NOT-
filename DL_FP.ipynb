{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***QT Database - Classification***"
      ],
      "metadata": {
        "id": "GlGX--Y7F10w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading database"
      ],
      "metadata": {
        "id": "PoXU8LsogLlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing WFDB, TensorFlow and Keras Tuner**\n",
        "\n",
        "The following command installs the `wfdb` Python library, which is required to read and process ECG signals from PhysioNet datasets such as the QT Database. This package provides tools for loading signals (`.dat`), metadata (`.hea`), and annotations (`.atr`)."
      ],
      "metadata": {
        "id": "SoPzVvOJGqt-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-apKomIg5E6"
      },
      "outputs": [],
      "source": [
        "!pip install wfdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade tensorflow keras-tuner"
      ],
      "metadata": {
        "id": "mABUUt0pMs1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mounting Google Drive and Uploading datasets**"
      ],
      "metadata": {
        "id": "MU2Oe4BCHF7b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2eEyYPyhPEY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGnElI9ShSBJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "qt_db_path = '/content/drive/MyDrive/DL_Project/qt-database-1.0.0/qt-database-1.0.0/'\n",
        "files = os.listdir(qt_db_path)\n",
        "print(files[:8])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QT Database Overview\n",
        "\n",
        "The **QT Database (QTDB)** is a collection of 105 ECG recordings, each identified by a code of the form `selnnnn`, where `nnnn` corresponds to the original record number from the source databases.\n",
        "\n",
        "Each recording contains **two ECG leads**:\n",
        "- **MLII (Modified Lead II)**\n",
        "- **V1 (precordial lead V1)**\n",
        "\n",
        "Each signal has a duration of **approximately 15 minutes**, sampled at **250 Hz**, resulting in around **225,000 samples per signal**.\n",
        "\n",
        "---\n",
        "\n",
        "### Per-record Files (Up to 9 per patient)\n",
        "\n",
        "For each recording, the dataset may include up to **9 files**, with the following suffixes:\n",
        "\n",
        "| Suffix  | Description |\n",
        "|---------|-------------|\n",
        "| `.hea`  | **Header file** — describes format and content of the signal |\n",
        "| `.dat`  | **Signal file** — binary data of the two ECG leads |\n",
        "| `.atr`  | **Reference beat annotations** from original database (not always available) |\n",
        "| `.man`  | **Manual annotations** of selected beats |\n",
        "| `.q1c`  | **Waveform boundaries (2nd pass)** from annotator 1 |\n",
        "| `.q2c`  | **Waveform boundaries (2nd pass)** from annotator 2 (only 11 records) |\n",
        "| `.qt1`  | **Waveform boundaries (1st pass)** from annotator 1 |\n",
        "| `.qt2`  | **Waveform boundaries (1st pass)** from annotator 2 (only 11 records) |\n",
        "| `.pu`   | **Automatic waveform boundaries** from both leads |\n",
        "| `.pu0`  | **Automatic waveform boundaries** from signal 0 (MLII) |\n",
        "| `.pu1`  | **Automatic waveform boundaries** from signal 1 (V1) |"
      ],
      "metadata": {
        "id": "ilg1vsvAKMoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here a list of the most common libraries we used:"
      ],
      "metadata": {
        "id": "8jaky455EK8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wfdb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "o8KKV6q0EI44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Visualizations"
      ],
      "metadata": {
        "id": "HqgsdipcfZAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visual Inspection of Full ECG Recordings\n",
        "\n",
        "The plots below show the full-length ECG recording (≈15 minutes) for patient **sel853** from the QT Database.  \n",
        "Both leads (**MLII** and **V1**) are displayed separately for clear comparison.\n",
        "\n",
        "Use the interactive slider to:\n",
        "-  Zoom into specific intervals\n",
        "-  Navigate across the entire signal duration\n",
        "\n",
        "This tool provides a direct visual exploration of beat morphology, rhythm, and signal quality in the original data.\n"
      ],
      "metadata": {
        "id": "qXQ6yLTpO5jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import interactive, IntRangeSlider\n",
        "from IPython.display import display\n",
        "\n",
        "# Load ECG\n",
        "record_id   = 'sel853' # Change this if you want to inspect another record\n",
        "record_path = os.path.join(qt_db_path, record_id)\n",
        "\n",
        "record = wfdb.rdrecord(record_path)\n",
        "signal = record.p_signal\n",
        "fs     = record.fs if hasattr(record, 'fs') else 250\n",
        "duration_sec = signal.shape[0] // fs\n",
        "t = np.arange(signal.shape[0]) / fs\n",
        "\n",
        "# Function to visualize ECG in an interactive time range\n",
        "def plot_ecg_window(time_range):\n",
        "    start_sec, end_sec = time_range\n",
        "    start_idx = int(start_sec * fs)\n",
        "    end_idx = int(end_sec * fs)\n",
        "\n",
        "    ti = t[start_idx:end_idx]\n",
        "    sig0 = signal[start_idx:end_idx, 0]\n",
        "    sig1 = signal[start_idx:end_idx, 1]\n",
        "\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(ti, sig0, color='blue')\n",
        "    plt.title(f'{record_id} — MLII')\n",
        "    plt.ylabel('Amplitude (mV)')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(ti, sig1, color='orange')\n",
        "    plt.title(f'{record_id} — V1')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude (mV)')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Interactive time range slider\n",
        "range_slider = IntRangeSlider(\n",
        "    value=[0, 10],\n",
        "    min=0,\n",
        "    max=duration_sec,\n",
        "    step=1,\n",
        "    description='Interval (s)',\n",
        "    continuous_update=False,\n",
        "    layout={'width': '80%'}\n",
        ")\n",
        "\n",
        "interactive_plot = interactive(plot_ecg_window, time_range=range_slider)\n",
        "display(interactive_plot)\n"
      ],
      "metadata": {
        "id": "lXGYiDrLQhIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Files Used for Classification\n",
        "\n",
        "For our classification task, we rely on the following four core files from each record in the QT Database:\n",
        "\n",
        "- **`.hea` (Header file)**: Contains metadata such as the number of signals, sampling frequency, signal names, and other configuration details.\n",
        "- **`.dat` (Signal file)**: Stores the raw ECG signals in binary format, typically including two leads (MLII and V1).\n",
        "- **`.atr` (Annotation file)**: Provides the R-peak positions (beat annotations) used as reference points to segment heartbeats.\n",
        "- **`.pu0` and `.pu1` (Waveform boundary files)**: Contain automatically detected waveform boundaries for signals 0 and 1. We use them only to verify the correctness of the `.atr` beat annotations and ensure alignment with actual beat locations.\n",
        "\n",
        "These files are sufficient for our purpose: heartbeat classification based on the raw ECG traces and annotated beat positions.\n",
        "\n",
        "Now let’s see how the `.hea` file is structured and what it contains:\n"
      ],
      "metadata": {
        "id": "7nz0xhHaTq4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "record_id = 'sel853'\n",
        "record_path = os.path.join(qt_db_path, record_id)\n",
        "\n",
        "# Load header (.hea) information\n",
        "record = wfdb.rdrecord(record_path)\n",
        "\n",
        "print(f\"\\n Header Information for Record: {record_id}\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Number of signals      : {record.n_sig}\")\n",
        "print(f\"Sampling frequency     : {record.fs} Hz\")\n",
        "print(f\"Duration               : {record.sig_len} samples ({record.sig_len / record.fs:.2f} seconds)\")\n",
        "print(f\"Base date              : {record.base_date}\")\n",
        "print(f\"Base time              : {record.base_time}\")\n",
        "print(f\"Signal names           : {record.sig_name}\")\n",
        "print(f\"Signal units           : {record.units}\")\n",
        "print(f\"ADC gain per signal    : {record.adc_gain}\")\n",
        "print(f\"Baseline values        : {record.baseline}\")\n",
        "print(f\"Checksum values        : {record.checksum}\")\n",
        "print(f\"Comments               : {record.comments}\")\n",
        "print(\"=\" * 60)\n",
        "\n"
      ],
      "metadata": {
        "id": "fr7TfvyFRSt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUm2NYiXh2li"
      },
      "source": [
        "### Visualizing ECG Signals with R-peak Annotations\n",
        "\n",
        "In this step, we load a full ECG record from the QT Database and visualize both channels (MLII and V1) along with the `.atr` annotations. These annotations indicate R-peak positions, which are important reference points for detecting heartbeats.\n",
        "\n",
        "The visualization helps verify the alignment between the raw signal and the annotated events, ensuring the integrity of the dataset before using it for classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyR4x7BjJSra"
      },
      "outputs": [],
      "source": [
        "# Specify the record to load\n",
        "record_id   = 'sel853'\n",
        "record_path = f'{qt_db_path}{record_id}'\n",
        "\n",
        "# 1) Load ECG signal and .atr annotations\n",
        "record  = wfdb.rdrecord(record_path)\n",
        "atr_ann = wfdb.rdann(record_path, 'atr')\n",
        "\n",
        "samples = atr_ann.sample\n",
        "symbols = atr_ann.symbol\n",
        "\n",
        "# 2) Define visualization interval\n",
        "start, end = 0, 3000  # samples (adjust as needed)\n",
        "\n",
        "# 3) Create figure with two subplots\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
        "\n",
        "# Plot MLII on the first axis\n",
        "ax1.plot(range(start, end), record.p_signal[start:end, 0], label='ECG MLII')\n",
        "ax1.set_ylabel('mV')\n",
        "ax1.set_title(f'Record {record_id} – MLII with .atr annotations')\n",
        "ax1.grid()\n",
        "for s in samples:\n",
        "    if start <= s < end:\n",
        "        ax1.plot(s, record.p_signal[s, 0], 'ro',\n",
        "                 label='R-peak' if 'R-peak' not in ax1.get_legend_handles_labels()[1] else \"\")\n",
        "ax1.legend(loc='upper right')\n",
        "\n",
        "# Plot V1 on the second axis\n",
        "ax2.plot(range(start, end), record.p_signal[start:end, 1], label='ECG V1', color='tab:orange')\n",
        "ax2.set_xlabel('Sample')\n",
        "ax2.set_ylabel('mV')\n",
        "ax2.set_title(f'Record {record_id} – V1 with .atr annotations')\n",
        "ax2.grid()\n",
        "for s in samples:\n",
        "    if start <= s < end:\n",
        "        ax2.plot(s, record.p_signal[s, 1], 'ro',\n",
        "                 label='R-peak' if 'R-peak' not in ax2.get_legend_handles_labels()[1] else \"\")\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Automatically Detected P-Wave Onsets (.pu0 and .pu1)\n",
        "\n",
        "In this step, we visualize the P-wave onset annotations automatically generated by the `ecgpuwave` algorithm, using the `.pu0` and `.pu1` files from the QT Database.\n",
        "\n",
        "- `.pu0` contains waveform onset and offset annotations for channel MLII (signal 0).\n",
        "- `.pu1` contains similar annotations for channel V1 (signal 1).\n",
        "\n",
        "We display a 12-second segment of the ECG signals for both channels, highlighting the automatically detected P-wave onsets.\n"
      ],
      "metadata": {
        "id": "v4UIQPPDZHGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the record to load\n",
        "record_id   = 'sel853'\n",
        "record_path = f'{qt_db_path}{record_id}'\n",
        "\n",
        "# 1) Read signal and .pu annotations\n",
        "record = wfdb.rdrecord(record_path)\n",
        "pu0_ann = wfdb.rdann(record_path, 'pu0')  # channel 0 (MLII)\n",
        "pu1_ann = wfdb.rdann(record_path, 'pu1')  # channel 1 (V1)\n",
        "pu0_samples = pu0_ann.sample\n",
        "pu0_symbols = pu0_ann.symbol\n",
        "pu1_samples = pu1_ann.sample\n",
        "pu1_symbols = pu1_ann.symbol\n",
        "\n",
        "# 2) Define the display interval\n",
        "start, end = 12000, 13000  # ~12 seconds window\n",
        "\n",
        "# 3) Create figure with two subplots\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
        "\n",
        "# Plot MLII (channel 0)\n",
        "ax1.plot(range(start, end), record.p_signal[start:end, 0], label='ECG MLII')\n",
        "ax1.set_ylabel('mV')\n",
        "ax1.set_title(f'Record {record_id} – MLII with .pu0 annotations')\n",
        "ax1.grid()\n",
        "for s in pu0_samples:\n",
        "    if start <= s < end:\n",
        "        ax1.plot(s, record.p_signal[s, 0], 'bo',\n",
        "                 label='P-onset (.pu)' if 'P-onset (.pu)' not in ax1.get_legend_handles_labels()[1] else \"\")\n",
        "ax1.legend(loc='upper right')\n",
        "\n",
        "# Plot V1 (channel 1)\n",
        "ax2.plot(range(start, end), record.p_signal[start:end, 1], label='ECG V1', color='tab:orange')\n",
        "ax2.set_xlabel('Sample')\n",
        "ax2.set_ylabel('mV')\n",
        "ax2.set_title(f'Record {record_id} – V1 with .pu1 annotations')\n",
        "ax2.grid()\n",
        "for s in pu1_samples:\n",
        "    if start <= s < end:\n",
        "        ax2.plot(s, record.p_signal[s, 1], 'bo',\n",
        "                 label='P-onset (.pu)' if 'P-onset (.pu)' not in ax2.get_legend_handles_labels()[1] else \"\")\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "chG_fNVBZYhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing Manual R-Peak Annotations (.atr) with Automatically Detected P-Wave Onsets (.pu)\n",
        "\n",
        "In this step, we compare the `.atr` annotations—manually labeled R-peaks—with the `.pu0` and `.pu1` files, which contain P-wave onset positions automatically detected using the `ecgpuwave` algorithm.\n",
        "\n",
        "By plotting both annotation types over the ECG signals from MLII and V1, we can visually validate the consistency and correctness of the `.atr` file. This comparison provides a layer of quality assurance before using the annotations in downstream classification tasks.\n"
      ],
      "metadata": {
        "id": "SZMoUxuSa7Zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the record to load\n",
        "record_id = 'sel853'\n",
        "record_path = f'{qt_db_path}{record_id}'\n",
        "\n",
        "# 1) Load signal and annotations from .atr, .pu0, and .pu1\n",
        "record = wfdb.rdrecord(record_path)\n",
        "atr_ann = wfdb.rdann(record_path, 'atr')   # R-peak annotations from .atr\n",
        "pu0_ann = wfdb.rdann(record_path, 'pu0')   # P-wave onsets for MLII from .pu0\n",
        "pu1_ann = wfdb.rdann(record_path, 'pu1')   # P-wave onsets for V1 from .pu1\n",
        "\n",
        "# 2) Extract all R-peaks (not just 'N' type)\n",
        "all_peaks = [s for s in atr_ann.sample]\n",
        "\n",
        "# 3) Extract P-wave onsets from .pu0 and .pu1\n",
        "p_onsets_mlii = pu0_ann.sample\n",
        "p_onsets_v1 = pu1_ann.sample\n",
        "\n",
        "# Display interval (first 1000 samples)\n",
        "start, end = 12000, 13000\n",
        "\n",
        "# Create figure with two subplots\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
        "\n",
        "# 4) Plot MLII (channel 0)\n",
        "ax1.plot(range(start, end), record.p_signal[start:end, 0], label='ECG MLII', color='blue')\n",
        "ax1.set_ylabel('mV')\n",
        "ax1.set_title(f'Record {record_id} – MLII with .atr and .pu0 annotations')\n",
        "ax1.grid()\n",
        "\n",
        "# Flags for legend entries\n",
        "peak_legend_added = False\n",
        "p_legend_added = False\n",
        "\n",
        "# Plot R-peaks from .atr on MLII\n",
        "for s in all_peaks:\n",
        "    if start <= s < end:\n",
        "        ax1.plot(s, record.p_signal[s, 0], 'ro', label='R-peak (.atr)' if not peak_legend_added else \"\")\n",
        "        peak_legend_added = True\n",
        "\n",
        "# Plot P-onsets from .pu0 on MLII\n",
        "for s in p_onsets_mlii:\n",
        "    if start <= s < end:\n",
        "        ax1.plot(s, record.p_signal[s, 0], 'bo', label='P-onset (.pu0)' if not p_legend_added else \"\")\n",
        "        p_legend_added = True\n",
        "\n",
        "ax1.legend(loc='upper right')\n",
        "\n",
        "# 5) Plot V1 (channel 1)\n",
        "ax2.plot(range(start, end), record.p_signal[start:end, 1], label='ECG V1', color='orange')\n",
        "ax2.set_xlabel('Sample')\n",
        "ax2.set_ylabel('mV')\n",
        "ax2.set_title(f'Record {record_id} – V1 with .atr and .pu1 annotations')\n",
        "ax2.grid()\n",
        "\n",
        "# Reset flags\n",
        "peak_legend_added = False\n",
        "p_legend_added = False\n",
        "\n",
        "# Plot R-peaks from .atr on V1\n",
        "for s in all_peaks:\n",
        "    if start <= s < end:\n",
        "        ax2.plot(s, record.p_signal[s, 1], 'ro', label='R-peak (.atr)' if not peak_legend_added else \"\")\n",
        "        peak_legend_added = True\n",
        "\n",
        "# Plot P-onsets from .pu1 on V1\n",
        "for s in p_onsets_v1:\n",
        "    if start <= s < end:\n",
        "        ax2.plot(s, record.p_signal[s, 1], 'bo', label='P-onset (.pu1)' if not p_legend_added else \"\")\n",
        "        p_legend_added = True\n",
        "\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "etzs4yS6avnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the figure above, we visualize both the manually annotated **R-peaks** (red dots, from `.atr`) and the automatically detected **P-wave onsets** (blue dots, from `.pu0`/`.pu1`) over the MLII and V1 ECG leads for patient `sel853`.\n",
        "\n",
        "While the red and blue markers do not always fall on *exactly* the same sample, they are consistently located around the apex of the QRS complex—where the R-peak typically occurs. This visual alignment suggests that the automatic detection is reasonably accurate for our use case.\n",
        "\n",
        "In the next steps, we'll explain how we handled and refined these annotations for training our classification model.\n"
      ],
      "metadata": {
        "id": "TPvcBh4TbjJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Data Preparation\n",
        "\n"
      ],
      "metadata": {
        "id": "uTwkEyZhfsQT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP7aKO7SRGoy"
      },
      "source": [
        "### 1. Verifying Missing `.atr` Files\n",
        "\n",
        "As mentioned in the official documentation and referenced papers describing the QT Database, not all patient records include a corresponding `.atr` annotation file. In particular, `.atr` files are missing for the 24 records associated with sudden cardiac death cases.\n",
        "\n",
        "Since our classification work relies on the `.atr` files for reference annotations (e.g., R-peak positions), it is important to identify which records are missing this file type and remove them from the dataset.\n",
        "\n",
        "In the following steps, we:\n",
        "1. Automatically scan all records to find those that are missing `.atr` files.\n",
        "2. Remove those incomplete records from the dataset to ensure only usable data remains.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUboXEdQQ5F6"
      },
      "outputs": [],
      "source": [
        "# Get all record names (without extension) in the folder\n",
        "all_files = [f.split(\".\")[0] for f in os.listdir(qt_db_path) if f.endswith('.dat')]\n",
        "\n",
        "# List of records missing a corresponding .atr file\n",
        "files_without_atr = []\n",
        "\n",
        "# Check if the .atr file exists for each record\n",
        "for record in all_files:\n",
        "    atr_file = os.path.join(qt_db_path, f'{record}.atr')\n",
        "    if not os.path.exists(atr_file):  # If the .atr file does not exist\n",
        "        files_without_atr.append(record)\n",
        "\n",
        "# Print records without associated .atr\n",
        "print(\"Files without corresponding .atr:\")\n",
        "for file in files_without_atr:\n",
        "    print(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB2XpNfDRDsP"
      },
      "outputs": [],
      "source": [
        "# List of files without the .atr file\n",
        "files_without_atr = [\n",
        "    'sel33', 'sel35', 'sel43', 'sel46', 'sel51', 'sel34', 'sel48', 'sel39',\n",
        "    'sel45', 'sel30', 'sel47', 'sel44', 'sel37', 'sel40', 'sel36', 'sel52',\n",
        "    'sel32', 'sel49', 'sel42', 'sel31', 'sel41', 'sel50', 'sel38', 'sel100'\n",
        "]\n",
        "\n",
        "# Delete the .dat and .hea files for records missing the .atr file\n",
        "for record in files_without_atr:\n",
        "    dat_file = os.path.join(qt_db_path, f'{record}.dat')\n",
        "    hea_file = os.path.join(qt_db_path, f'{record}.hea')\n",
        "\n",
        "    # Check if the files exist and remove them\n",
        "    if os.path.exists(dat_file):\n",
        "        os.remove(dat_file)\n",
        "        print(f\"Deleted file: {dat_file}\")\n",
        "\n",
        "    if os.path.exists(hea_file):\n",
        "        os.remove(hea_file)\n",
        "        print(f\"Deleted file: {hea_file}\")\n",
        "\n",
        "print(\"Cleanup completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Exploring Beat Annotation Symbols\n",
        "\n",
        "Before labeling the heartbeats as **normal** or **abnormal**, it's important to verify all the possible values that the `.atr` annotation files can contain.\n",
        "\n",
        "Each `.atr` file includes a list of symbols associated with detected events in the ECG signal. These symbols can represent different types of beats or markers.\n",
        "\n",
        "Let's extract and print all the **unique annotation symbols** found across the entire QT database.\n"
      ],
      "metadata": {
        "id": "7n8kg73KABGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all records that have a .hea file\n",
        "records = [f.split('.')[0] for f in os.listdir(qt_db_path) if f.endswith('.hea')]\n",
        "\n",
        "# Set to collect all unique annotation symbols\n",
        "unique_symbols = set()\n",
        "\n",
        "for record_id in records:\n",
        "    record_path = os.path.join(qt_db_path, record_id)\n",
        "\n",
        "    try:\n",
        "        ann = wfdb.rdann(record_path, 'atr')  # Read .atr annotation file\n",
        "        unique_symbols.update(ann.symbol)    # Add symbols to the set\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {record_id} due to error: {e}\")\n",
        "\n",
        "print(f\"✅ Unique annotation symbols found in the QTDB: {sorted(unique_symbols)}\")\n"
      ],
      "metadata": {
        "id": "O7p6ys3f81hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Annotation Symbols in QTDB (for Beat Classification)\n",
        "\n",
        "| Symbol | Description                              | Label to Use        |\n",
        "|--------|------------------------------------------|---------------------|\n",
        "| N      | Normal beat                              |  Normal           |\n",
        "| L      | Left bundle branch block beat            |  Abnormal         |\n",
        "| R      | Right bundle branch block beat           |  Abnormal         |\n",
        "| A      | Atrial premature beat                    |  Abnormal         |\n",
        "| a      | Aberrated atrial premature beat          |  Abnormal         |\n",
        "| J      | Nodal (junctional) beat                  |  Abnormal         |\n",
        "| S      | Supraventricular premature beat          |  Abnormal         |\n",
        "| V      | Premature ventricular contraction (PVC)  |  Abnormal         |\n",
        "| F      | Fusion of ventricular and normal beat    |  Abnormal         |\n",
        "| e      | Atrial escape beat                       |  Abnormal         |\n",
        "| j      | Nodal escape beat                        |  Abnormal         |\n",
        "| f      | Fusion of paced and normal beat          |  Abnormal         |\n",
        "| s      | Supraventricular escape beat             |  Abnormal         |\n",
        "| Q      | Unknown beat                             |  Abnormal         |\n",
        "| /      | Non-beat marker (e.g., segment boundary) |  Ignore           |\n",
        "| +      | Rhythm change annotation                 |  Ignore           |\n",
        "| ~      | Signal artifact or noise                 |  Ignore           |\n",
        "| \"      | Comment (not a beat)                     |  Ignore           |\n",
        "| \\|      | Isolated marker                          |  Ignore           |\n"
      ],
      "metadata": {
        "id": "GLTQKuWO93Pg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Simplified `.atm` Annotations for Classification\n",
        "\n",
        "In this step, we generate a simplified annotation file (`.atm`) for each ECG record based on the original `.atr` annotations. Here's what the script does:\n",
        "\n",
        "1. **Loads all records with a `.hea` header file** to identify valid entries.\n",
        "2. **Loops through each record** and reads both the signal and the `.atr` annotations.\n",
        "3. **Filters the annotations** by keeping only alphabetic beat labels (A-Z), which correspond to actual heartbeat types.\n",
        "   - The label `'N'` (Normal beat) is kept as is.\n",
        "   - All other beat types are relabeled as `'A'` (Abnormal), creating a binary classification setup.\n",
        "4. **Saves a new `.atm` annotation file** only if valid beats are found, using the simplified binary labels.\n",
        "5. **Logs** the result for each record, indicating whether a new file was created or skipped.\n",
        "\n",
        "This preprocessing step standardizes the annotation format across all records and prepares the data for training a binary classifier (Normal vs Abnormal).\n"
      ],
      "metadata": {
        "id": "At7rFvJM5v-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all records with a .hea file\n",
        "records_with_hea = sorted([\n",
        "    f.split('.')[0] for f in os.listdir(qt_db_path) if f.endswith('.hea')\n",
        "])\n",
        "\n",
        "# Loop through all records\n",
        "for record_id in records_with_hea:\n",
        "    record_path = os.path.join(qt_db_path, record_id)\n",
        "\n",
        "    try:\n",
        "        # Read signal and .atr annotations\n",
        "        record = wfdb.rdrecord(record_path)\n",
        "        atr_ann = wfdb.rdann(record_path, 'atr')\n",
        "\n",
        "        filtered_samples = []\n",
        "        filtered_symbols = []\n",
        "\n",
        "        # Filter symbols: keep only alphabetic ones (A-Z)\n",
        "        for s, sym in zip(atr_ann.sample, atr_ann.symbol):\n",
        "            if sym.isalpha():  # Only alphabetic symbols\n",
        "                if sym == 'N':\n",
        "                    filtered_symbols.append('N')  # Normal\n",
        "                else:\n",
        "                    filtered_symbols.append('A')  # Any other letter = Abnormal\n",
        "                filtered_samples.append(int(s))  # Ensure base int conversion\n",
        "\n",
        "        # Save new .atm file only if valid beats were found\n",
        "        if len(filtered_samples) > 0:\n",
        "            wfdb.wrann(\n",
        "                record_id,\n",
        "                'atm',\n",
        "                sample=np.array(filtered_samples, dtype=int),  #Convert to int ndarray\n",
        "                symbol=filtered_symbols,\n",
        "                write_dir=qt_db_path\n",
        "            )\n",
        "\n",
        "            new_atr_path = os.path.join(qt_db_path, f'{record_id}.atm')\n",
        "            print(f\" {record_id}: File saved ➜ Filtered beats: {len(filtered_samples)}\")\n",
        "\n",
        "        else:\n",
        "            print(f\" {record_id}: No valid beats found, skipping .atm save\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error in {record_id}: {e}\")\n",
        "\n",
        "print(\"\\n Processing completed for all records!\")\n"
      ],
      "metadata": {
        "id": "ZLzQJpeL5odu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Why Does Patient `sel102` Have Only 1 Saved Beat?***"
      ],
      "metadata": {
        "id": "4PxBlSULj5kT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "record_id = 'sel102'\n",
        "record_path = os.path.join(qt_db_path, record_id)\n",
        "\n",
        "ann = wfdb.rdann(record_path, 'atr')\n",
        "\n",
        "print(f\"\\nUnique symbols in {record_id}: {set(ann.symbol)}\")\n",
        "print(f\"Total original annotations: {len(ann.symbol)}\")\n",
        "\n",
        "# Count how many are letters\n",
        "letter_symbols = [s for s in ann.symbol if s.isalpha()]\n",
        "print(f\"Total isalpha() annotations: {len(letter_symbols)}\")\n",
        "\n",
        "# Show the first 50 original symbols\n",
        "print(f\"First 50 symbols: {ann.symbol[:50]}\")"
      ],
      "metadata": {
        "id": "u9KbFQ2CjazE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**  \n",
        "In the case of `sel102`, we see that almost all annotations are `/` (a non-beat technical marker).  \n",
        "Only one annotation (`'V'`) corresponds to a heartbeat.  \n",
        "\n",
        "Since we only retain symbols that are **alphabetic characters (A–Z)** for beat classification,  \n",
        "it's expected that only **one valid beat** is kept.\n"
      ],
      "metadata": {
        "id": "muYx7WnyByTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see another example:"
      ],
      "metadata": {
        "id": "TQPXVHawB9_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path\n",
        "record_id = 'sel104'\n",
        "record_path = f'/content/drive/MyDrive/DL_Project/qt-database-1.0.0/qt-database-1.0.0/{record_id}'\n",
        "\n",
        "# Load original .atr annotations\n",
        "atr_ann = wfdb.rdann(record_path, 'atr')\n",
        "\n",
        "# Load filtered .atm annotations\n",
        "atm_ann = wfdb.rdann(record_path, 'atm')\n",
        "\n",
        "# ----- Compare annotation symbols -----\n",
        "print(f\"Record: {record_id}\")\n",
        "\n",
        "# Original .atr info\n",
        "print(\"\\n--- Original (.atr) ---\")\n",
        "print(f\"Unique symbols: {sorted(set(atr_ann.symbol))}\")\n",
        "print(f\"Total annotations: {len(atr_ann.symbol)}\")\n",
        "print(f\"First 50 symbols: {atr_ann.symbol[:50]}\")\n",
        "\n",
        "# Filtered .atm info\n",
        "print(\"\\n--- Filtered (.atm) ---\")\n",
        "print(f\"Unique symbols: {sorted(set(atm_ann.symbol))}\")\n",
        "print(f\"Total annotations: {len(atm_ann.symbol)}\")\n",
        "print(f\"First 50 filtered symbols: {atm_ann.symbol[:50]}\")\n"
      ],
      "metadata": {
        "id": "_Li3Oj4LEuVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Window-based Labeling for ECG Classification\n",
        "\n",
        "In this step, we segment each ECG recording into 2-second windows and assign a label to each window based on the R-peaks it contains.\n",
        "\n",
        "For each window:\n",
        "- We detect the R-peaks using `.atr` annotations.\n",
        "- We match each R-peak to its corresponding label (`N` for normal, or any other symbol marked as abnormal) from the `.atm` file.\n",
        "- If at least one abnormal beat is found in the window, the entire window is labeled as **Abnormal**; otherwise, it is labeled as **Normal**.\n",
        "\n",
        "The goal is to generate a labeled dataset of fixed-size ECG segments, ready for training and evaluation in the classification model.\n",
        "\n",
        "This step also logs any unmatched R-peaks and exports the full list of labeled windows to a CSV file for downstream tasks.\n"
      ],
      "metadata": {
        "id": "iWwmmbcQGXV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic parameters\n",
        "fs = 250  # Hz\n",
        "window_size = 2  # seconds\n",
        "samples_per_window = window_size * fs\n",
        "match_tolerance = 10\n",
        "total_missing = 0\n",
        "\n",
        "# Data directory\n",
        "data_dir = '/content/drive/MyDrive/DL_Project/qt-database-1.0.0/qt-database-1.0.0/'\n",
        "\n",
        "# Find all records with .hea files\n",
        "records_with_q1c = sorted([\n",
        "    f.split('.')[0] for f in os.listdir(data_dir) if f.endswith('.hea')\n",
        "])\n",
        "\n",
        "windows_results = []\n",
        "\n",
        "# Function to find matching annotation within tolerance\n",
        "def find_label(r_peak, atm_samples, atm_symbols, tolerance=3):\n",
        "    for s, sym in zip(atm_samples, atm_symbols):\n",
        "        if abs(s - r_peak) <= tolerance:\n",
        "            return sym\n",
        "    return None  # No match found\n",
        "\n",
        "# Loop through each record\n",
        "for record_id in records_with_q1c:\n",
        "    record_path = os.path.join(data_dir, record_id)\n",
        "\n",
        "    try:\n",
        "        atr_ann = wfdb.rdann(record_path, 'atr')\n",
        "        r_peaks = atr_ann.sample\n",
        "\n",
        "        atm_ann = wfdb.rdann(record_path, 'atm')\n",
        "        atm_samples = atm_ann.sample\n",
        "        atm_symbols = atm_ann.symbol\n",
        "\n",
        "        record = wfdb.rdrecord(record_path)\n",
        "        signal = record.p_signal\n",
        "\n",
        "        for start_sample in range(0, len(signal) - samples_per_window + 1, samples_per_window):\n",
        "            end_sample = start_sample + samples_per_window\n",
        "\n",
        "            r_peaks_in_window = [r for r in r_peaks if start_sample <= r < end_sample]\n",
        "\n",
        "            window_labels = []\n",
        "            for r_peak in r_peaks_in_window:\n",
        "                label = find_label(r_peak, atm_samples, atm_symbols, tolerance=match_tolerance)\n",
        "\n",
        "                if label is None:\n",
        "                    total_missing += 1\n",
        "                    continue\n",
        "\n",
        "                if label == 'N':\n",
        "                    window_labels.append('Normal')\n",
        "                elif label == 'A':\n",
        "                    window_labels.append('Abnormal')\n",
        "\n",
        "            if window_labels:\n",
        "                window_label = 'Abnormal' if 'Abnormal' in window_labels else 'Normal'\n",
        "                windows_results.append({\n",
        "                    'record': record_id,\n",
        "                    'window_start': start_sample,\n",
        "                    'window_end': end_sample,\n",
        "                    'label': window_label,\n",
        "                    'r_peaks': r_peaks_in_window\n",
        "                })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" ERROR in {record_id}: {e}\")\n",
        "\n",
        "# Save and show statistics\n",
        "windows_df = pd.DataFrame(windows_results)\n",
        "\n",
        "print(f\"\\n TOTAL WINDOWS CREATED: {len(windows_results)}\")\n",
        "print(windows_df['label'].value_counts())\n",
        "print(f\" TOTAL R-peaks ignored: {total_missing}\")\n",
        "print(windows_df.head(30))\n",
        "\n",
        "# Save to CSV\n",
        "windows_df.to_csv('/content/record_windows.csv', index=False)\n"
      ],
      "metadata": {
        "id": "hM4tgfVhKiVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output shows the final statistics after labeling 2-second windows of ECG data based on beat annotations:\n",
        "\n",
        "Total Windows Created: **35690** — Each window contains one or more R-peaks and has been labeled either as Normal or Abnormal depending on the beats inside.\n",
        "\n",
        "- **Normal Windows: 32343**\n",
        "\n",
        "- **Abnormal Windows: 3347**\n",
        "\n",
        "- **Total R-peaks Ignored: 2,560**\n",
        "\n",
        "This means that 2,560 R-peaks from the .atr files did not match any beat in the .atm annotations (even within a ±10 sample tolerance). These unmatched peaks were skipped and not used for window labeling.\n",
        "\n",
        "\n",
        "The `.atm` files were created by filtering only alphabetic symbols (`A–Z`) from the original `.atr` annotations. Any non-alphabetic symbols (like `'/'`, `'+'`, or `'~'`) were excluded, because they are **not heartbeat annotations**.\n",
        "So, if some R-peaks were originally labeled with such symbols, they:\n",
        "\n",
        "- Were **not included** in the `.atm` file,  \n",
        "- And therefore, could **not be matched** during window labeling,  \n",
        "- Leading to them being **counted as ignored R-peaks**.\n",
        "\n",
        "This reinforces that the filtering step worked as intended — only valid, classifiable beats (Normal or Abnormal) were retained.\n",
        "\n"
      ],
      "metadata": {
        "id": "2o8zHIqxOOD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Visualizing Beat Labels from `.atm` Files**\n",
        "\n",
        "In this step, we visualize the ECG signal from both leads (MLII and V1) along with the annotated beat labels from the `.atm` file. These labels were previously filtered and classified as either `'N'` for normal or `'A'` for abnormal beats.\n",
        "\n",
        "The code extracts a 2-second window (500 samples at 250 Hz) from the beginning of the recording and overlays the annotations directly on the ECG trace. This allows for a clear view of:\n",
        "\n",
        "- The morphology of each beat  \n",
        "- The exact position of the annotation  \n",
        "- The label assigned to that beat\n",
        "- The label assigned to the window (on the following code)  \n",
        "\n",
        "This visual check helps verify that the annotation process worked correctly and that the retained beat labels align properly with the corresponding peaks in both channels.\n"
      ],
      "metadata": {
        "id": "wcxSIQYcUPTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the record to load\n",
        "record_id = 'sel103'  # Change this to the record you want to visualize\n",
        "record_path = os.path.join(data_dir, record_id)\n",
        "\n",
        "# 1) Load ECG signal\n",
        "record = wfdb.rdrecord(record_path)\n",
        "signal = record.p_signal\n",
        "print(\"Full signal shape:\", signal.shape)\n",
        "\n",
        "# 2) Load annotations\n",
        "atm_ann = wfdb.rdann(record_path, 'atm')\n",
        "atm_samples = atm_ann.sample\n",
        "atm_symbols = atm_ann.symbol\n",
        "\n",
        "# 3) Define window\n",
        "start_sample = 1000\n",
        "end_sample = start_sample + 2 * 250\n",
        "window_signal = signal[start_sample:end_sample, :]   # use both channels\n",
        "print(\"Window shape:\", window_signal.shape)\n",
        "\n",
        "# 4) Dual plot\n",
        "fig, axs = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
        "\n",
        "axs[0].plot(np.arange(start_sample, end_sample), window_signal[:, 0], label='MLII', color='blue')\n",
        "axs[0].set_title(f\"{record_id} - MLII\")\n",
        "axs[0].set_ylabel(\"Amplitude\")\n",
        "axs[0].grid(True)\n",
        "for sample, symbol in zip(atm_samples, atm_symbols):\n",
        "    if start_sample <= sample < end_sample:\n",
        "        y0 = window_signal[sample - start_sample, 0]\n",
        "        axs[0].plot(sample, y0, 'ro')\n",
        "        axs[0].text(sample, y0, symbol, color='red', fontsize=10)\n",
        "\n",
        "axs[1].plot(np.arange(start_sample, end_sample), window_signal[:, 1], label='V1', color='orange')\n",
        "axs[1].set_title(f\"{record_id} - V1\")\n",
        "axs[1].set_xlabel(\"Samples\")\n",
        "axs[1].set_ylabel(\"Amplitude\")\n",
        "axs[1].grid(True)\n",
        "for sample, symbol in zip(atm_samples, atm_symbols):\n",
        "    if start_sample <= sample < end_sample:\n",
        "        y1 = window_signal[sample - start_sample, 1]\n",
        "        axs[1].plot(sample, y1, 'ro')\n",
        "        axs[1].text(sample, y1, symbol, color='red', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7suIVIebTYAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look the label of this window:"
      ],
      "metadata": {
        "id": "1NPzZMbFv2Zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find R-peaks within the window\n",
        "r_peaks_in_window = [s for s in atm_samples if start_sample <= s < end_sample]\n",
        "\n",
        "labels_in_window = []\n",
        "for s in r_peaks_in_window:\n",
        "    idx = list(atm_samples).index(s)\n",
        "    labels_in_window.append(atm_symbols[idx])\n",
        "\n",
        "# Force conversion of each element to integer\n",
        "r_peaks_in_window = [int(x) for x in r_peaks_in_window]\n",
        "print(\" R-peak positions:\", r_peaks_in_window)\n",
        "\n",
        "print(\" Labels of the peaks:\", labels_in_window)\n",
        "\n",
        "# Determine window label\n",
        "if labels_in_window:\n",
        "    if 'A' in labels_in_window:\n",
        "        window_label = 'Abnormal'\n",
        "    else:\n",
        "        window_label = 'Normal'\n",
        "        print(f\"\\n Final label for this window: {window_label}\")\n",
        "else:\n",
        "    print(\"\\n Window discarded: no beats inside.\")\n",
        "    window_label = None"
      ],
      "metadata": {
        "id": "YlGjjbDMwUG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This output confirmed that our idea works.**"
      ],
      "metadata": {
        "id": "yn22XGB_HoHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##  Data Preprocessing\n"
      ],
      "metadata": {
        "id": "QUXu0KRPfHRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading and Preparation\n",
        "\n",
        "In this section, we load the data from the CSV file where the ECG windows were saved. We removed the `label` column from the **windows** dataset because we use it as the target variable **y**. The **X** variable will contain the features extracted from the ECG windows.\n",
        "\n",
        "The following code demonstrates how to load the data, separate the features **X** and labels **y**, and display the shape of the loaded data.\n"
      ],
      "metadata": {
        "id": "s6phFGzQ9evn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the data from the CSV file where windows were saved\n",
        "import pandas as pd\n",
        "\n",
        "# Load the windows data\n",
        "windows_df = pd.read_csv('/content/record_windows.csv')\n",
        "\n",
        "# Separate features (X) and labels (y)\n",
        "X = windows_df.drop(columns=['label']).values  # Features\n",
        "y = windows_df['label'].values  # Labels\n",
        "\n",
        "print(\"Data loaded:\", X.shape, y.shape)\n"
      ],
      "metadata": {
        "id": "nq6CV8-FaNL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Splitting and Normalization\n",
        "\n",
        "This section of the code performs the following steps:\n",
        "\n",
        "1. **Load the data**: The ECG windows are loaded from a CSV file, and features (X) and labels (y) are separated.\n",
        "2. **Preprocessing**: The ECG data is filtered using a notch filter to remove 50Hz power line interference and a band-pass filter to retain relevant ECG frequencies (0.5–40Hz).\n",
        "3. **Data Splitting**: The data is split into **Training**, **Validation**, and **Test** sets using a stratified approach to maintain class distribution.\n",
        "4. **Normalization**: Z-score normalization is applied only to the **Training set**. The same normalization is then applied to the **Validation** and **Test** sets.\n",
        "5. **Save Preprocessed Data**: The normalized data is saved in a compressed `.npz` file for future use.\n",
        "\n",
        "The data distribution before and after normalization is printed for each set.\n"
      ],
      "metadata": {
        "id": "Vynzd95OySod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.signal import butter, filtfilt, iirnotch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Paths\n",
        "data_dir    = '/content/drive/MyDrive/DL_Project/qt-database-1.0.0/qt-database-1.0.0/'\n",
        "windows_csv = '/content/record_windows.csv'\n",
        "out_npz     = '/content/drive/MyDrive/X_y_preprocessed.npz'\n",
        "\n",
        "# 1) Check if the .npz file already exists\n",
        "if os.path.exists(out_npz):\n",
        "    print(\"Preprocessed data loaded from disk.\")\n",
        "    data = np.load(out_npz)\n",
        "    X_train, X_val, X_test = data['X_train'], data['X_val'], data['X_test']\n",
        "    y_train, y_val, y_test = data['y_train'], data['y_val'], data['y_test']\n",
        "    print(\"Shape X_train:\", X_train.shape, \"Shape y_train:\", y_train.shape)\n",
        "else:\n",
        "    print(\"No preprocessed data found. Running preprocessing...\")\n",
        "\n",
        "    # 2) Define preprocessing function\n",
        "    def preprocess_ecg(sig, fs=250):\n",
        "        # Notch filter at 50Hz to remove power line interference\n",
        "        b_n, a_n = iirnotch(50, 30, fs)\n",
        "        sig = filtfilt(b_n, a_n, sig)\n",
        "        # Band-pass filter between 0.5–40Hz to keep ECG relevant frequencies\n",
        "        b_bp, a_bp = butter(4, [0.5/(fs/2), 40/(fs/2)], btype='band')\n",
        "        return filtfilt(b_bp, a_bp, sig)\n",
        "\n",
        "    # 3) Load window table from CSV\n",
        "    windows_df = pd.read_csv(windows_csv)\n",
        "\n",
        "    X_list, y_list = [], []\n",
        "\n",
        "    fs = 250  # Sampling frequency\n",
        "\n",
        "    # 4) Loop over each row in windows_df, filter and normalize each window\n",
        "    for _, row in tqdm(windows_df.iterrows(), total=len(windows_df), desc=\"Preprocessing + Z-score\"):\n",
        "        rec   = row['record']\n",
        "        start = int(row['window_start'])\n",
        "        end   = int(row['window_end'])\n",
        "        lbl   = row['label']\n",
        "\n",
        "        # Read the ECG record\n",
        "        record = wfdb.rdrecord(os.path.join(data_dir, rec))\n",
        "        sig0   = record.p_signal[start:end, 0]\n",
        "        sig1   = record.p_signal[start:end, 1]\n",
        "\n",
        "        # Apply filters to ECG channels\n",
        "        f0 = preprocess_ecg(sig0, fs)\n",
        "        f1 = preprocess_ecg(sig1, fs)\n",
        "\n",
        "        # Stack channels → shape (500, 2)\n",
        "        w = np.stack([f0, f1], axis=1)\n",
        "        assert w.shape == (500, 2), f\"Invalid window shape: {w.shape}\"\n",
        "\n",
        "        # Save the filtered window and its corresponding label\n",
        "        X_list.append(w)\n",
        "        y_list.append(0 if lbl == 'Normal' else 1)\n",
        "\n",
        "    # 5) Convert to final arrays (stack the windows)\n",
        "    X = np.stack(X_list)\n",
        "    y = np.array(y_list)\n",
        "\n",
        "    # 6) Split the data into train, validation, and test sets (stratified)\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.10, random_state=42, stratify=y)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.111111, random_state=42, stratify=y_temp)\n",
        "\n",
        "    print(\"Training set (80%) distribution:\")\n",
        "    print(f\"Normal: {np.sum(y_train == 0)}\")\n",
        "    print(f\"Abnormal: {np.sum(y_train == 1)}\")\n",
        "    print(\"\\nValidation set (10%) distribution:\")\n",
        "    print(f\"Normal: {np.sum(y_val == 0)}\")\n",
        "    print(f\"Abnormal: {np.sum(y_val == 1)}\")\n",
        "    print(\"\\nTest set (10%) distribution:\")\n",
        "    print(f\"Normal: {np.sum(y_test == 0)}\")\n",
        "    print(f\"Abnormal: {np.sum(y_test == 1)}\")\n",
        "\n",
        "    # 7) Apply Z-score normalization only to the training set\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Calcola la normalizzazione sul training set (solo sui dati di addestramento)\n",
        "    X_train_reshaped = X_train.reshape(-1, 2)  # Flatten per calcolare la normalizzazione su tutte le finestre\n",
        "    scaler.fit(X_train_reshaped)  # Calcola media e deviazione standard sui dati di addestramento\n",
        "\n",
        "    # Applica la normalizzazione ai dati di addestramento\n",
        "    X_train_normalized = scaler.transform(X_train_reshaped).reshape(X_train.shape)\n",
        "\n",
        "    # Applica la stessa normalizzazione ai dati di validazione e di test\n",
        "    X_val_normalized = scaler.transform(X_val.reshape(-1, 2)).reshape(X_val.shape)\n",
        "    X_test_normalized = scaler.transform(X_test.reshape(-1, 2)).reshape(X_test.shape)\n",
        "\n",
        "    # 8) Save the normalized data to disk in a compressed .npz format\n",
        "    np.savez(out_npz, X_train=X_train_normalized, X_val=X_val_normalized, X_test=X_test_normalized,\n",
        "             y_train=y_train, y_val=y_val, y_test=y_test)\n",
        "\n",
        "    print(\"Preprocessing + normalization completed and saved to disk.\")\n",
        "    print(\"Shape X_train:\", X_train_normalized.shape, \"Shape y_train:\", y_train.shape)\n",
        "    print(\"Shape X_val:\", X_val_normalized.shape, \"Shape y_val:\", y_val.shape)\n",
        "    print(\"Shape X_test:\", X_test_normalized.shape, \"Shape y_test:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "lZ6J_GWBJiej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look the results:"
      ],
      "metadata": {
        "id": "Or8AcQH5yuh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Path to the preprocessed file\n",
        "out_npz = '/content/drive/MyDrive/X_y_preprocessed.npz'\n",
        "\n",
        "# Load the preprocessed data\n",
        "data = np.load(out_npz)\n",
        "\n",
        "# Access the arrays stored in the file\n",
        "X_train = data['X_train']\n",
        "X_val = data['X_val']\n",
        "X_test = data['X_test']\n",
        "y_train = data['y_train']\n",
        "y_val = data['y_val']\n",
        "y_test = data['y_test']\n",
        "\n",
        "# Print shapes to verify\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_val: {X_val.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_val: {y_val.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")\n",
        "\n",
        "# You can also inspect the first example\n",
        "print(\"First example of X_train:\", X_train[0])\n",
        "print(\"First label of y_train:\", y_train[0])\n"
      ],
      "metadata": {
        "id": "JRBlqCfwDn_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Data Augmentation and Training Set Balancing**\n",
        "\n",
        "  To address the imbalance between normal and abnormal windows in the training set, we perform two key steps:\n",
        "\n",
        "    1️. **Data Augmentation**  \n",
        "    We generate synthetic abnormal samples by applying random transformations to the original abnormal windows:\n",
        "      - **Jittering**: Adding small Gaussian noise to simulate sensor variability.\n",
        "      - **Scaling**: Randomly amplifying or attenuating the signal to introduce realistic variation.\n",
        "\n",
        " 2️. **Balancing the Dataset**  \n",
        "    After augmenting the abnormal windows, we balance the training set by:\n",
        "      - Combining original and augmented abnormal windows.\n",
        "      - **Undersampling** the normal windows to match the number of abnormal samples.\n",
        "      - Shuffling the data to avoid order bias.\n",
        "\n",
        "This procedure ensures a **balanced dataset (50% normal, 50% abnormal)**, which helps the model learn equally from both classes and prevents bias toward the majority class.\n"
      ],
      "metadata": {
        "id": "qxCq1hJ94ccv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Augmentation function\n",
        "def augment_signal(window, jitter_sigma=0.05, scale_sigma=0.1):\n",
        "    augmented = np.copy(window)\n",
        "    # Jitter (additive Gaussian noise)\n",
        "    augmented += np.random.normal(0.0, jitter_sigma, size=augmented.shape)\n",
        "    # Scaling (multiplicative Gaussian noise)\n",
        "    scale = np.random.normal(1.0, scale_sigma, size=(1, augmented.shape[1]))\n",
        "    augmented *= scale\n",
        "    return augmented\n",
        "\n",
        "# Balancing + augmentation\n",
        "def balance_training(X_train, y_train, factor_abn=2):\n",
        "    # Select the indices for normal and abnormal windows\n",
        "    idx_norm = np.where(y_train == 0)[0]\n",
        "    idx_abn  = np.where(y_train == 1)[0]\n",
        "\n",
        "    print(f\"Original Abnormal: {len(idx_abn)}\")\n",
        "    print(f\"Original Normal: {len(idx_norm)}\")\n",
        "\n",
        "    # 1️) Generate augmented abnormal windows\n",
        "    augmented_abn = []\n",
        "    for _ in range(len(idx_abn) * factor_abn):  # Augmentation of abnormal windows\n",
        "        w = X_train[np.random.choice(idx_abn)]\n",
        "        w_aug = augment_signal(w)\n",
        "        augmented_abn.append(w_aug)\n",
        "    augmented_abn = np.stack(augmented_abn)\n",
        "\n",
        "    # 2️) Combine original and augmented abnormal windows\n",
        "    X_abn_final = np.concatenate([X_train[idx_abn], augmented_abn])\n",
        "    y_abn_final = np.ones(len(X_abn_final))\n",
        "\n",
        "    print(f\"Total abnormal: {len(idx_abn)} original + {len(augmented_abn)} augmented = {len(X_abn_final)}\")\n",
        "\n",
        "    # 3️) Undersample the normal windows to match the size of the abnormal windows\n",
        "    undersample_size = len(X_abn_final)\n",
        "    undersampled_norm = X_train[np.random.choice(idx_norm, size=undersample_size, replace=False)]\n",
        "    y_norm_final = np.zeros(undersample_size)\n",
        "\n",
        "    # 4️) Combine and shuffle the data\n",
        "    X_train_bal = np.concatenate([X_abn_final, undersampled_norm])\n",
        "    y_train_bal = np.concatenate([y_abn_final, y_norm_final])\n",
        "\n",
        "   # Random shuffling\n",
        "    perm = np.random.permutation(len(y_train_bal))\n",
        "    X_train_bal = X_train_bal[perm]\n",
        "    y_train_bal = y_train_bal[perm]\n",
        "\n",
        "    print(f\"Balanced training set: {len(y_train_bal)} total | Normal={sum(y_train_bal==0)}, Abnormal={sum(y_train_bal==1)}\")\n",
        "    return X_train_bal, y_train_bal\n",
        "\n",
        "# 5) Run the balancing function\n",
        "X_train_bal, y_train_bal = balance_training(X_train, y_train, factor_abn=3)\n",
        "\n",
        "# 6) Check the final class distribution\n",
        "unique, counts = np.unique(y_train_bal, return_counts=True)\n",
        "print(\"\\nFinal balanced training distribution:\")\n",
        "for cls, count in zip(unique, counts):\n",
        "    label = 'Normal' if cls == 0 else 'Abnormal'\n",
        "    perc = 100 * count / len(y_train_bal)\n",
        "    print(f\" -> {label}: {count} windows ({perc:.2f}%)\")\n",
        "print(f\"Total windows: {len(y_train_bal)}\")\n"
      ],
      "metadata": {
        "id": "lY9Yylg5Mk9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing Data Augmentation Effects**\n",
        "\n",
        "In this step, we compare an **original abnormal window** from the training set with its **augmented version**."
      ],
      "metadata": {
        "id": "uj9OG6ss8MAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select an abnormal window from the balanced training set\n",
        "idx_abn_bal = np.where(y_train_bal == 1)[0]\n",
        "\n",
        "# Select the abnormal indices in the new balanced training set\n",
        "idx = idx_abn_bal[8000]  # Choose a specific example (for example, the 8000th)\n",
        "original = X_train_bal[idx]\n",
        "augmented = augment_signal(original)\n",
        "\n",
        "# Visualization of the comparison between the original window and the augmented one\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(original[:, 0], label='MLII original', color='blue')\n",
        "plt.plot(original[:, 1], label='V1 original', color='orange')\n",
        "plt.title(\"Original\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(augmented[:, 0], label='MLII augmented', color='blue')\n",
        "plt.plot(augmented[:, 1], label='V1 augmented', color='orange')\n",
        "plt.title(\"Augmented\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3tZ_G62iNAaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This visualization allows us to verify that the augmented signal remains realistic, while introducing controlled perturbations to improve model generalization.\n"
      ],
      "metadata": {
        "id": "kgo2iTRV8YqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Hyperparameter Optimization with Keras Tuner\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KH0BcIdy0jpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we perform hyperparameter optimization on the **ECG classification model** using **Keras Tuner**. The goal is to find the best combination of hyperparameters for the model, ensuring optimal performance for the task.\n",
        "\n",
        "The model consists of three **Conv1D layers** for feature extraction, followed by **Global Average Pooling** and a **Dense layer** for classification. Key hyperparameters being tuned include:\n",
        "\n",
        "1. **Filters and kernel sizes** for the convolutional layers.\n",
        "2. **Number of units** in the dense layer.\n",
        "3. **L2 regularization** for controlling overfitting.\n",
        "4. **Dropout rate** to prevent overfitting during training.\n",
        "5. **Learning rate** for the Adam optimizer.\n",
        "\n",
        "We use **RandomSearch** for hyperparameter tuning, with the objective of maximizing **validation recall**. The search is performed over 30 trials, with 50 epochs per trial and early stopping for efficiency.\n",
        "\n",
        "The following code sets up the hyperparameter search and trains the model with the best configurations found.\n"
      ],
      "metadata": {
        "id": "xlEiTR7u3jX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import keras_tuner as kt\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Set input shape\n",
        "input_shape = (500, 2)\n",
        "\n",
        "# Function for building the model\n",
        "def build_model(hp):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # First Conv1D layer\n",
        "    model.add(layers.Conv1D(\n",
        "        filters=hp.Int('conv1_filters', min_value=64, max_value=256, step=64),  # Expanded filter size\n",
        "        kernel_size=hp.Choice('conv1_kernel', [3, 5, 7, 11]),  # Expanded kernel size\n",
        "        padding='same',\n",
        "        input_shape=input_shape\n",
        "    ))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.MaxPooling1D(2))\n",
        "\n",
        "    # Second Conv1D layer\n",
        "    model.add(layers.Conv1D(\n",
        "        filters=hp.Int('conv2_filters', min_value=128, max_value=256, step=64),  # Expanded filter size\n",
        "        kernel_size=hp.Choice('conv2_kernel', [3, 5, 7]),  # Expanded kernel size\n",
        "        padding='same'\n",
        "    ))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.MaxPooling1D(2))\n",
        "\n",
        "    # Third Conv1D layer\n",
        "    model.add(layers.Conv1D(\n",
        "        filters=hp.Int('conv3_filters', min_value=128, max_value=256, step=64),  # Expanded filter size\n",
        "        kernel_size=hp.Choice('conv3_kernel', [3, 5, 7]),  # Expanded kernel size\n",
        "        padding='same'\n",
        "    ))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.MaxPooling1D(2))\n",
        "\n",
        "    # Global Average Pooling and Dense layers\n",
        "    model.add(layers.GlobalAveragePooling1D())\n",
        "    model.add(layers.Dense(\n",
        "        units=hp.Int('dense_units', min_value=128, max_value=256, step=64),  # Expanded dense units\n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.l2(\n",
        "            hp.Float('l2', min_value=1e-5, max_value=1e-3, sampling='log')\n",
        "        )\n",
        "    ))\n",
        "\n",
        "    # Dropout layer with expanded search range\n",
        "    model.add(layers.Dropout(hp.Float('dropout', min_value=0.2, max_value=0.6, step=0.1)))  # Increased range\n",
        "\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Float('lr', min_value=1e-4, max_value=1e-2, sampling='log')),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall'), tf.keras.metrics.AUC(name='AUC')]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Keras Tuner setup\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective=kt.Objective('val_recall', direction='max'),\n",
        "    max_trials=30,  # Increased number of trials\n",
        "    executions_per_trial=1,\n",
        "    directory='tuning_dir',\n",
        "    project_name='ecg_tuning_extended'\n",
        ")\n",
        "\n",
        "# Summary of search space\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# Start the tuning process with the extended parameters\n",
        "tuner.search(\n",
        "    X_train_bal, y_train_bal,\n",
        "    epochs=50,  # Increased the number of epochs\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=64,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_recall', mode='max', patience=10, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Results summary\n",
        "tuner.results_summary()\n"
      ],
      "metadata": {
        "id": "ALaCSR9XYDHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture and Training\n"
      ],
      "metadata": {
        "id": "upJcaspl00aV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we define and train a **static model** with optimized hyperparameters obtained from the previous tuning process. The model consists of three **Conv1D layers** for feature extraction, followed by **Global Average Pooling** and **Dense layers** for classification.\n",
        "\n",
        "Key characteristics of the model include:\n",
        "- **Conv1D layers** with optimized filter sizes and kernel sizes to capture relevant features from the ECG signals.\n",
        "- **BatchNormalization** and **LeakyReLU** activations for better convergence and performance.\n",
        "- **Dropout** to reduce overfitting during training.\n",
        "- **L2 regularization** applied to the dense layer to prevent overfitting.\n",
        "- **Learning rate** has been optimized for the Adam optimizer.\n",
        "\n",
        "The model is compiled with **binary cross-entropy loss** and evaluated using metrics like **accuracy**, **precision**, **recall**, and **AUC**.\n",
        "\n",
        "The training process includes **early stopping**, **learning rate reduction on plateau**, and **model checkpointing** to save the best performing model.\n"
      ],
      "metadata": {
        "id": "O8gfJmKq3qlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import numpy as np\n",
        "\n",
        "# 1) Static model with optimized hyperparameters (from Trial 28)\n",
        "input_shape = (500, 2)\n",
        "model = models.Sequential([\n",
        "    # First Conv1D layer: optimized filters=64, kernel_size=5\n",
        "    layers.Conv1D(64, 5, padding='same', input_shape=input_shape),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.LeakyReLU(),\n",
        "    layers.MaxPooling1D(2),\n",
        "\n",
        "    # Second Conv1D layer: optimized filters=256, kernel_size=7\n",
        "    layers.Conv1D(256, 7, padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.LeakyReLU(),\n",
        "    layers.MaxPooling1D(2),\n",
        "\n",
        "    # Third Conv1D layer: optimized filters=192, kernel_size=3\n",
        "    layers.Conv1D(192, 3, padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.LeakyReLU(),\n",
        "    layers.MaxPooling1D(2),\n",
        "\n",
        "    # Global Average Pooling\n",
        "    layers.GlobalAveragePooling1D(),\n",
        "\n",
        "    # Dense layer with optimized units=192 and L2 regularization\n",
        "    layers.Dense(\n",
        "        192,\n",
        "        activation='relu',\n",
        "        kernel_regularizer=regularizers.l2(1.5409404207370287e-05)\n",
        "    ),\n",
        "    # Dropout layer with dropout=0.2\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 2) Compile the model with an optimal learning rate (lr=0.009644270859916792)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.009644270859916792),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall'),\n",
        "        tf.keras.metrics.AUC(name='AUC')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 3) Set up robust callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', mode='min', patience=8, restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss', factor=0.5, patience=4\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        'best_model_final.keras', monitor='val_loss', save_best_only=True\n",
        "    )\n",
        "]\n",
        "\n",
        "# 4) Train the model with the balanced training set\n",
        "history = model.fit(\n",
        "    X_train_bal, y_train_bal,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "# Print final training history summary\n",
        "print(\"Training history:\", history.history)\n"
      ],
      "metadata": {
        "id": "z84MuLeX66eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "cV3T1fXR9b6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation: Test Set Results"
      ],
      "metadata": {
        "id": "lDJ-ATkxAibd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this final step, we evaluate the trained model on the independent test set, analyzing performance through the confusion matrix, classification report, AUC score, precision-recall trade-off across different thresholds and ROC curve.\n"
      ],
      "metadata": {
        "id": "pIdxXkNt3vzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_auc_score,\n",
        "    precision_recall_curve\n",
        ")\n",
        "import seaborn as sns\n",
        "\n",
        "# Predict\n",
        "y_pred_proba = model.predict(X_test)\n",
        "threshold = 0.3\n",
        "y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(f\"Confusion Matrix (thr={threshold})\")\n",
        "plt.show()\n",
        "\n",
        "# Report e AUC\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Normal','Abnormal']))\n",
        "print(f\"AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
        "\n",
        "# Precision-Recall vs Threshold\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(thresholds, precision[:-1], label='Precision')\n",
        "plt.plot(thresholds, recall[:-1], label='Recall')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Precision & Recall vs Threshold')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Mu-PXuNk-rJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Calculate ROC curve and AUC\n",
        "fpr, tpr, thresholds_roc = roc_curve(y_test, y_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC Curve\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate (Recall)')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QjZIPHdMYv1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save the model in a .h5 file\n",
        "\n",
        "model.save('/content/drive/MyDrive/DL_Project/ecg_classification_model.h5')\n",
        "print(\"Modello salvato in /content/drive/MyDrive/DL_Project/ecg_classification_model.h5\")\n"
      ],
      "metadata": {
        "id": "Dj7yStJem5xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "OkIIkdiTMIHH"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
